<!doctype html>
<html lang="en">

<head>
  <!-- Required meta tags -->
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

  <title>  On the Statistical Detection of Demographic Biases in College Admissions | Stats and Bonuses
</title>
  <link rel="canonical" href="/admissions-bias.html">


  <link rel="stylesheet" href="/theme/css/bootstrap.min.css">
  <link rel="stylesheet" href="/theme/css/font-awesome.min.css">
  <link rel="stylesheet" href="/theme/css/pygments/default.min.css">
  <link rel="stylesheet" href="/theme/css/theme.css">

  
  <meta name="description" content="Ways to test for negative effects of demographic filtering">


</head>

<body>
  <header class="header">
    <div class="container">
<div class="row">
    <div class="col-sm-4">
      <a href="/">
        <img class="img-fluid rounded" src=/images/keima.jpg alt="Stats and Bonuses">
      </a>
    </div>
  <div class="col-sm-8">
    <h1 class="title"><a href="/">Stats and Bonuses</a></h1>
      <p class="text-muted">Jim's blog on data and everything else | 🔮💰👑🤔💻📷🍁🌌🔬🏹</p>
      <ul class="list-inline">
            <li class="list-inline-item"><a href="/pages/about.html">About Me</a></li>
            <li class="list-inline-item"><a href="/pages/admissions.html">College Admissions Counselling</a></li>
      </ul>
  </div>
</div>    </div>
  </header>

  <div class="main">
    <div class="container">
      <h1>  On the Statistical Detection of Demographic Biases in College Admissions
</h1>
      <hr>
  <article class="article">
    <header>
      <ul class="list-inline">
        <li class="list-inline-item text-muted" title="2018-09-02T23:52:00-07:00">
          <i class="fa fa-clock-o"></i>
          2018.09.02
        </li>
        <li class="list-inline-item">
          <i class="fa fa-folder-open-o"></i>
          <a href="/category/admissions.html">Admissions</a>
        </li>
      </ul>
    </header>
    <div class="content">
      <p>The methodology I outline for bias detection is more general than just demographics or college admissions, but given the  <a href="https://www.cnn.com/2018/08/30/politics/harvard-justice-department-affirmative-action-asian-americans-lawsuit/index.html">recent</a> and upcoming events surrounding Harvard’s Affirmative Action <a href="https://www.bbc.com/news/world-us-canada-45360044">AA policy</a>, this context provides a relatable medium to describe some approaches I would take to perform statistical detection of anomalies. Note that while the linked news article mentions the Department of Justice providing a stance on the matter, this may stem from Trump’s stance on <a href="https://www.bbc.com/news/world-us-canada-44703874">AA</a>.</p>
<p>We will walk through several hypothetical Caltech admissions datasets to investigate a suspicion regarding its admissions process.</p>
<p>Usually, an institution’s (liberal) politically correct official stance for quotas is something along these lines: “We are committed to maintaining diversity of thought by considering <em>trait X</em> in our holistic admissions process. We are an affirmative action, equal-opportunity institution…” Or, in rarer cases, it could be “No, we don’t do affirmative action during <em>admissions</em>. We simply have an improved ratio of <em>trait X</em> because in recent years, we have <em>encouraged a more diverse pool of applicants to apply</em>.” Harvard is clearly the former, and Caltech seems to be the latter.</p>
<p>The goal is <strong>not to</strong> <strong>judge</strong> whether the institution’s stance is <em>good</em> or even <em>morally correct</em>. We only care about detecting what <em>is</em> and what <em>is not</em> occurring.</p>
<p>For all approaches, assume:</p>
<ol>
<li>Any data on admissions are provided by the school on an ask-for basis.</li>
<li>All relevant documents relating to the admissions process are preserved.</li>
<li>The premise for “fairness” is “better academic record means better chance of admission, regardless of demographic identity”.</li>
<li>We are only allowed to use past data; no <a href="https://www.aeaweb.org/articles?id=10.1257/0002828042002561">experiments with future applicants</a> allowed.</li>
</ol>
<p>Some key questions to be addressed for applicants, all other qualifications held constant:</p>
<ol>
<li>Using data<em> only from the admitted</em> students, is the school favouring a group with <em>trait X</em> and causing a negative side-effect on <em>evaluation metric</em> (think SAT score, GPA, etc.)? If ‘yes’, the answer to question #2 below is certainly ‘No.’ If ‘no’, then q#2 is worth investigating.</li>
<li>Using data <em>from all</em> applicants, is the probability of admission, <em>conditional</em> on having vs. not having <em>trait X</em>, the same? In other words, is the process numerically "race-blind"?</li>
<li>Using data <em>from all</em> applicants, is there an effective <em>target quota</em> set by the school on the distribution of <em>trait X</em> over the admitted students?</li>
<li>Using data <em>only from the admitted</em> students, is there an effective <em>target quota</em> set by the school on the distribution of <em>trait X</em> over the admitted students?</li>
</ol>
<p><strong>Scenario</strong>: A few years ago, Caltech publicly denied all styles of affirmative action. On a first glance at the historical data, we are suspicious of this claim - suppose we want to find: <strong>is there affirmative action at Caltech favouring gender F sent on CommonApp?</strong> This scenario also generalises to the SFFA v. Harvard case - just switch ‘CommonApp gender’ to ‘CommonApp race’.</p>
<p><strong>Background.</strong>
This scenario is quite relevant to Caltech: in recent years, we have seen a rapid improvement (in my opinion) of the gender ratio for the undergraduate class. While some testosterone-fuelled problems are mitigated, one possible method by which this demographic change was effected raises a problem: to many women - at Caltech especially - knowing that they were admitted not “just because of their gender” matters significantly, and a betrayal of trust in the admissions committee to be impartial to demographics (as it seems to have claimed at some point in the <a href="http://qr.ae/TUNMnZ">past</a>) would be a scandal. It would allow for male students and future employers to berate Caltech female graduates by saying, “You got in just because you’re a woman.”</p>
<p>Nasty, yet also immediately applicable to racial affirmative action.</p>
<p>My observations on this matter can be represented by the <a href="http://qr.ae/TUNMnZ">Quora quote</a> below:</p>
<blockquote>
<p>I am a female who chose Caltech and not MIT specifically because Caltech did not consider my gender in the admissions process. Caltech has managed to significantly increase their female attendance rate without resorting to unbalanced admissions processes. Therefore, no one can ever say that I got anything because I am a woman.
- <a href="https://www.quora.com/profile/Kelly-Martin-26">Kelly Martin</a>, studied at California Institute of Technology</p>
</blockquote>
<p>Let’s start with an obvious approach regarding test scores, inspired by complaints featured in the SFFA v. Harvard case. Since Caltech is known to be more of a meritocracy than almost all other US schools, there should be little room for excuse if we find that there is a large difference in test scores for the <strong>F</strong> and <strong>M</strong> populations.</p>
<h2>Approach #1, a test for negative side-effects of demographic balancing:</h2>
<p>“Let’s see the distribution of academic scores for <em>only the admitted</em> students, separated into groups <strong>F</strong> and <strong>M</strong>. If one group’s distribution sits significantly far apart from the other group’s, then something fishy is going on. If they are close enough, then at least we know, no negative side-effects to this academic score are existent.”</p>
<p>By <em>distribution</em>, I mean a function mapping [measurement] into [observation rate (density)]. One can visualise a distribution by converting the observations into a sort of histogram like <a href="https://en.wikipedia.org/wiki/Histogram">this</a>.</p>
<p>So, we plot a distribution of standardised (in-sample) <a href="https://en.wikipedia.org/wiki/Standard_score#Comparison_of_scores_measured_on_different_scales:_ACT_and_SAT">ACT/SAT</a> scores for each group. Example of standardisation calculation: Booty’s ACT score was 35. The [mean, std] of <em>all admitted</em> students’ ACT scores is [34.5, 0.5]. By <em>mean</em>, I mean “sum everything up and divide by the number of samples”. By <em>std</em>, I mean the <a href="https://en.wikipedia.org/wiki/Standard_deviation"><em>standard deviation</em></a>, which is a measure of how precise our <em>mean</em> is. Booty’s standardised ACT <em>“z”-score</em> is calculated:</p>
<p><span class="math">\(\frac{score_{Booty} - \mu_{scores}}{\sigma{scores}} = \frac{35 - 34.5}{0.5} = 1\)</span></p>
<p>Corresponding to 1 std. above the mean ACT score. The result is pretty intuitive, since the mean is 34.5, and 1 std. is 0.5 points, and Booty got a 35. The ‘z’ in <em>z-score</em> is not exactly correct, but ignore that for now. We perform a similar calculation for the SAT scores, and get <em>z-scores</em> for that as well. This makes it so that we can compare both ACT and SAT on the same scale!</p>
<p>Afterwards, we use the F and M <em>z-score</em> distributions to calculate the <strong>probability that the difference</strong> in population mean scores <strong>occurs “just by chance”</strong>. In an ideal no-bias world, the z-score distribution means should have zero difference - in fact, the z-score distributions should look the same. We can test for distribution sameness using the <a href="https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test">Kolmogorov–Smirnov test</a> (plot the two cumulative distributions and look at the largest difference). In the real world, any deviation in the means can be explained away by “random chance”.</p>
<p>The probability of this ‘random chance’ argument being valid shrinks as the observed <em>std</em> of each distribution decreases (the measurements are more certain) and as the difference of means increases (the measurements are more damning). The smaller this probability is, then the less believable that explanation is, and the more “statistically significant” a finding is. This is what (questionable) scientists mean when they keep saying “My <strong><em>p-value</em></strong> is less than 0.05.” They mean, according to their distribution of results, the <strong>probability of their finding occurring due to random chance</strong> is less than 5%. This probability can be analytically calculated through a variety of statistical tests, in this case the <a href="https://en.wikipedia.org/wiki/Student%27s_t-test"><em>t-test</em></a> might be a good choice. Or, preferably, it could be numerically calculated through Bayesian methods - more on this later. A “good” scientist should always report <em>all</em> weaknesses in their statistical calculations, and we will try to criticise how this <em>p-value</em> might be erroneous:</p>
<p><em>Criticisms:</em></p>
<ol>
<li><em>What if group <strong><em>*F</em></strong></em> only takes the SAT and group <strong><em>*M</em></strong><em> only takes he ACT? Your z-scores would be created by only intra-group means and std’s.</em></li>
<li><em>What if the SAT has a nonlinear rating scale causing a non-Gaussian distribution of scores?</em></li>
<li><em>Academics isn’t just about your ACT/SAT score!</em></li>
</ol>
<p><em>Answers:</em></p>
<ol>
<li>Exactly. To remedy this, we will calculate the true <em>z-scores</em> using the<strong> </strong>entire population of SAT/ACT takers <strong>in the world.</strong><ol>
<li><em>Follow-up criticism: What if SAT takers all over the world are inherently smarter than ACT takers?</em></li>
<li>Answer: I certainly felt this way as I chickened out of memorising the pre-2016 SAT vocabulary. Instead, I “power-farmed” the historical ACT exams over a week. In case this hunch is true, we should look at SAT and ACT distributions independently, making for a total of 4 sub-populations to compare. If the <em>entire </em>hypothetical scenario is true (<em>both </em>criticisms 1 <em>and</em> 1a), then we should discard SAT and ACT entirely in our analysis, or use some calibration curve between the two.</li>
</ol>
</li>
<li>
<p>We should not do the z-score calculation unless the SAT and ACT distributions are both Gaussian. If the distributions look nasty from the start, we can perform Bayesian analysis using <a href="https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm">MCMC</a> to simulate our way through calculating the probability of anomalies “happening by chance”. Essentially, we perform numerical integrals after numerically simulating a posterior distribution. No cookbook statistical tests are involved, very few assumptions, only empirical evidence!</p>
<ol>
<li><em>Follow-up criticism: Simulations aren’t real.</em></li>
<li>Answer: Sure, but since we take many, many samples using real data, the simulation error versus an analytical calculation is tiny and insignificant.</li>
</ol>
</li>
<li>
<p>The nice thing about z-scores is that you can use them for any Gaussian-distributed rating - number of national academic awards, an “artificial” Caltech-calculated academic score, some non-academic scores, etc. SAT/ACT is just a familiar example. If the stuff we want to consider isn’t Gaussian, we can always try a <a href="https://en.wikipedia.org/wiki/Power_transform">transform</a> to make it Gaussian.</p>
</li>
</ol>
<p><strong>Additionally, we can use well-controlled (by major) “in-Caltech performance” metrics to detect negative side effects of balancing. After all, if high school performance isn’t indicative of Caltech performance, and the mission of the school is to produce good scientists in the future, then high school performance and SAT/ACT scores don’t mean anything. In fact, I prefer this metric of assessment, since I believe performance on future, unseen data is the only thing that matters.</strong></p>
<p><strong>Possible conclusions to be made at this point:</strong></p>
<ul>
<li>If the resulting distributions (z-score or just plain test scores) show strong discrepancies, something is fishy. For what reason does one group have higher qualifications than the other? The most common explanation for this goes, “There are just so many factors that go into admissions! It’s not all about your academic scores, there are also extracurriculars and awards, volunteering, essays, and <a href="https://admissionscase.harvard.edu/key-points">insert other traits here</a>” Occasionally, a statistical inquirer on a public forum voicing suspicions of the school’s claims will get the equivalent to a “Fuck off. You’re wrong. <a href="https://www.facebook.com/caltechconfessions/posts/1879016655723198?comment_id=1880867128871484&amp;reply_comment_id=1881021518856045">You think you can capture people in numbers?</a>” This will be addressed in the very beginning of <strong>Approach #2</strong>.</li>
<li>
<p>If the resulting distributions (z-score or just plain test scores) are approximately <a href="https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot">equal</a>, <strong>there is nothing to complain about</strong> in terms of student calibre or qualifications. This applies not just to gender, but any other demographic/multiclass trait as well (think race, athletic recruits, donors, faculty, etc.).</p>
<ul>
<li><strong>“Fairness” and “equal opportunity” are another story; consider the following example-</strong></li>
<li>100 of <strong>F</strong> and 200 of <strong>M </strong>apply to Caltech, They have the <strong>exact same qualifications</strong> and differ only in gender.</li>
<li>The school takes 100 of <strong>F</strong> and<strong> </strong>100 of <strong>M</strong> to get a balanced 50:50 gender ratio.</li>
<li>The acceptance rate for <strong>F</strong> is 100%, while <strong>M</strong> has a 50% acceptance rate. “BLASPHEMY”, some would say. However, the calibre of students remains the same, which is usually the only concern of your typical Caltech student. “Fairness” is more of a question of moral values and individual preferences.</li>
<li>If fairness is of concern, then one could calculate the probability of admission for each M and F, <strong><em>conditional on</em></strong> having the same qualifications (defined by the evaluator). This gets us into <strong>Approach #2.</strong></li>
</ul>
</li>
</ul>
<h2>Approach #2, a test for numerical "blindness":</h2>
<p>Using data <em>from all</em> applicants, is the probability of admission, <em>conditional</em> on having vs. not having <em>demographic trait X</em>, the same? In other words, <strong>is the process numerically "race-blind"?</strong> This approach requires data on <em>all applicants, not just the admits</em>. This is almost impossible to obtain in the real world without some hacking or leaking.</p>
<p>If Approach #1 was tried already and yielded positive results for demographic difference in evaluation metrics, then at this point the investigator is typically confronted with various attacks from the institution and its believers, with justifications and evasions of the following sort:</p>
<ol>
<li><em>“Do you happen to know what those people put on their apps? There is a concept of a balanced class and those women may have been chosen for things like their extracurricular or science interest, as opposed to their gender.”</em></li>
<li><em>“Applicants are evaluated holistically and academics alone isn’t all that we look at.”</em></li>
</ol>
<p>I have two responses to these ‘holistic’ arguments:</p>
<ol>
<li>
<p>Regardless of what happened with the rest of the applicant’s features, if the resulting distribution of some evaluation metric for the class, separated into <strong>M</strong> and <strong>F,</strong> is revealed to be unbalanced, then something must be inherently wrong with the evaluation process in terms of fairness. If admissions has been trusted to admit students <strong>by assessing their academic/scientific merit</strong>, and gender somehow came into play there, then the admissions committee is at fault. The <strong>intentions motivating the process is of no consequence</strong>, be it “balancing” or “diversity”; the damage has been done and any justification is up to the community to judge. Consider this case:</p>
<ol>
<li>Strip gender data altogether from the application from the very start, and metaphorically put them in a locked box until school starts. Also, <strong>strip data that is correlated to gender, such as names, as much as possible.</strong></li>
<li>Do the admissions process based on remaining data and churn out enrolled students. Don’t do balancing of any kind; only use a ranking process.</li>
<li>Reveal the gender data again.</li>
<li>Evaluate the statistical difference in academic or whatever metrics we want between M/F populations, either through an analytical approach or MCMC numerical methods.</li>
<li>It is <strong>very unlikely </strong>that we would find F vs M distributions for academic metrics to be very different, since we evaluated students on pure merit, unrelated to gender.</li>
</ol>
</li>
<li>
<p>“Balancing” a class without side-effects is one thing, but using demographics as a way to balance a class results in side-effects like having a distribution mismatch tested for by <strong>Approach #1</strong>. This kind of discrepancy is the basis for nasty comments like “Girls at Caltech are dumb.” <strong>We don’t want these comments to have any statistical grounding.</strong></p>
</li>
</ol>
<p><strong>The above points don’t address “fairness” or “equal chance”</strong>. We will look at them now. To assess whether the school promotes “equal chance”, we can build a prediction model. With Caltech, it is especially difficult since there are so few training samples, but let’s suppose we had 23500 students per year instead of 235. Steps:</p>
<ol>
<li>Split off a random 10% of the data (“validation set”) for use in model assessment at the very end. Do not touch this until we are satisfied.<ol>
<li>Otherwise, “leftist” media outlets like <a href="https://mediabiasfactcheck.com/slate/">Slate</a> will call it <a href="https://slate.com/technology/2018/06/harvards-discrimination-lawsuit-has-given-us-details-about-who-gets-in.html">“statistical double-dipping”</a>. <strong>Those pesky journalists will probably accuse us of that either way</strong>, given the politically biased track record of US media. I believe that journalists have the <strong>obligation and responsibility</strong> to convey accurate depictions of technical matters to the public, <a href="https://en.wikipedia.org/wiki/Journalism_ethics_and_standards">regardless of their political stance</a>. Therefore, they should shut their mouth on the topics of statistics and science, unless they have a solid grounding in the relevant knowledge. Or, news agencies could just hire better <a href="https://en.wikipedia.org/wiki/Technical_writer">technical writers</a> or <a href="https://www.economist.com/blogs/graphicdetail">data teams</a>.</li>
<li>This is not AA-specific; if we were to investigate something that appealed to the <em>political right</em>, then Fox News would come hunting us in the same fashion [think: Robert Mueller].</li>
<li>I am surprised that a journal like <em>The Economist</em>, often accused of being “leftist”, supports <a href="https://www.economist.com/leaders/2013/04/27/time-to-scrap-affirmative-action">colour-blind college admissions</a> Looks like the <em>American left</em> consists of a different breed of <em>left</em>.</li>
</ol>
</li>
<li>Choose features wisely and use a binary classifier that determines “admit” or “reject”.<ol>
<li><a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic regression</a> seems to be a popular choice among professors who show up <a href="https://www.scribd.com/document/382125856/Harvard-Plaintiff-Arcidiacono-Expert-Report">in court</a>, as well as <a href="http://samv91khoyt2i553a2t1s05i-wpengine.netdna-ssl.com/wp-content/uploads/2018/06/Doc-421-145-Admissions-Part-II-Report.pdf">Harvard’s internal researchers, pre-litigation</a>. After all, it does offer explainability with acceptable performance.</li>
<li>I surmise that XGBoost or something more complex than logistic regression would have the advantage of better classification accuracy if enough data were available, but complex models are usually less transparent. Of course, with the new <a href="https://github.com/slundberg/shap">SHAP tree explainer</a> <a href="https://slundberg.github.io/shap/notebooks/League%20of%20Legends%20Win%20Prediction%20with%20XGBoost.html">(link to sample with LoL win predictor)</a>, things are better. ML models are not good for litigation, but quite good for curious data scientists. Alternatively, we could also put the dataset on <a href="https://www.kaggle.com/">Kaggle</a> along with everyone’s social security number and credit card numbers, and take the best-performing model.</li>
</ol>
</li>
<li>Split the remaining 90% of data (“training data”) into <a href="https://king-jim.com/ml-tut2.html">k folds, let k be 5 or 10.</a></li>
<li>Train, test, tweak, train, test, tweak the model on the “training data” through k-fold Cross Validation until satisfied.</li>
<li>Evaluate the model on the “validation set”, and that tells us our final prediction score. The validation set <strong>is entirely new data to the model</strong> - we can treat it like a quiz or a new dataset entirely.</li>
<li>Using 100% of the dataset, perform a 10-fold Cross Validation using the same model from step #5. Report the [mean, std] accuracy rate or <a href="https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve">AUC</a> from the 10 trials as well.</li>
</ol>
<p>If our model scored sufficiently well (subjective), we perform uniformly random draws of the applicants, <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)">with replacement</a>.</p>
<ol>
<li>For every applicant we draw, we evaluate their <strong>probability</strong> of admission using our binary classifier from above. Save this in an array or a dataframe “array 1”</li>
<li>We modify their demographic data, and <strong>nothing else</strong>, into the other classes of <em>demographic trait</em>. In our case, this means M -&gt; F, and F -&gt; M.</li>
<li>Evaluate the hypothetical applicant from step #2 using our model. Save this in a second array or dataframe “array 2”. By generating hypothetical applicants by changing their demographic class, we attempting to conduct a virtual, controlled, randomised experiment, while still using real applicant data in the first array.</li>
<li>Repeat steps #1–3 until we go through every single applicant, or until we draw enough samples to satisfy our curiosity, whichever we want.</li>
<li>Using the two (or more, when it isn’t F vs. M) arrays or dataframes of hypothetical+real students’ admission rates, we generate distributions for each class of <em>demographic trait</em> by histograms.</li>
<li>If there is a discrepancy in the two distributions, then something is fishy. We perform a <em>t-test</em> if these distributions are each normal, but otherwise we will use numerical (MCMC) methods to calculate the <strong>probability that these admission rate distributions have different means “by random chance”</strong>.</li>
</ol>
<p>If <strong>little</strong> to no discrepancy is found in the admission rate distributions, then the process is “fair” with respect to the <em>demographic trait</em> we were analysing. Otherwise, there is reason to be suspicious of the “fairness” in admissions, <strong>even if there are no negative side-effects</strong> found in Approach # 1.</p>
<p>This is where things get tricky: what amount of discrepancy is <strong>little</strong>, especially since our prediction model is not perfect? That would be where things get philosophical and debatable.</p>
<h2>Approach #3, a test for the existence of an effective target quota, using all data.</h2>
<p>Using data <em>from all</em> applicants, this is pretty easy. If discovered, it is halfway-incriminating (<a href="https://en.wikipedia.org/wiki/Gratz_v._Bollinger">intentional quotas are illegal</a>), but showing that the effect is <em>intentional </em>and<em> explicit</em> is a nightmare.</p>
<ol>
<li><strong>Plot 1.</strong> We take a look at historical<em> applicant</em> population fractions, by <em>demographic trait</em>. For example, if 100 <strong>F</strong> and 200 <strong>M</strong> <em>applied</em>to Caltech in 2018, then the <strong>F</strong> fraction is 0.33 and the <strong>M</strong> fraction is 0.67. We plot these over time, one data point per year per demographic group.</li>
<li><strong>Plot 2.</strong> We look at the historical <em>enrolled </em>school population (without graduating seniors) fractions, by <em>demographic trait</em>. If 1000 <strong>F</strong> and 1000 <strong>M</strong> (no graduating seniors) were <em>enrolled</em> at Caltech in 2018, then the <strong>F</strong> fraction is 0.5 and the <strong>M</strong> fraction is 0.5. We plot these over time, one data point per year per demographic group. The idea is, if the school wants to maintain a quota, it should adjust according to the current demographic composition of the<em> remaining </em>class.</li>
<li><strong>Plot 3.</strong> Then we look at the historical <em>admitted </em>population fractions, by <em>demographic trait</em>. If 100 <strong>F</strong> and 100 <strong>M</strong> were <em>admitted</em> to Caltech in 2018, then the <strong>F</strong> fraction is 0.5 and the <strong>M</strong> fraction is 0.5. We plot these over time, one data point per year per demographic group.</li>
</ol>
<p>And we will see some pretty obvious things:</p>
<ol>
<li>If there is a target quota and no fluctuation in Plot #2, then Plot #3 will for sure hover around the same fractions, every single year. This is as boring of a <a href="https://en.wikipedia.org/wiki/Racial_quota">quota</a> as we can get.<ol>
<li>This is a necessary effect of a target quota, but there is no causality in the other direction; if there is no such quota, we might still observe this flat Plot #3. This is because Plot #1 could be flat, too, among other reasons.</li>
</ol>
</li>
<li>If we see the following, there is probably a target quota: Plot #2 fluctuates, and Plot #3 fluctuates <strong>in the opposite direction, exactly one year lagged from Plot #2.</strong> This is a footprint of compensation and balancing. It would be very hard to explain these weird co-movements without admitting to demographic balancing to meet a target. Example:<ol>
<li>In 2017, Plot #2 shows F=0.45 &amp; M=0.55 in enrolled. The target quota in the past has been (according to plot #2) [F, M]=[0.5, 0.5]. Thus, in 2018, the school needs to admit more F. This will be shown as a <strong>high value</strong> in F and a <strong>low value</strong> in M in Plot #3, something like F=0.6 &amp; M=0.4 in 2018 admits.</li>
</ol>
</li>
<li>If Plot #1 and Plot #3 are anti-correlated over time (increasing demographic group X as applicants, but decreasing demographic group X in admitted list), and Plot #2 is flat, something is probably very wrong.<ol>
<li><em>Criticism: what if simply, more people from group X have been applying lately, and they are of lower quality?</em></li>
<li>Answer: Yes, this would explain things, if true. To determine how true it is, we must do the following test. If we control (slice data) for ability using academic scores, and still see this effect, we can be more certain that there is a quota in place.</li>
</ol>
</li>
<li>If there is no target quota, Plot #3 should move together with Plot #1. This is based on the (very reasonable) assumption that demographic group quality doesn’t change <em>that much</em> in quality over one or two years. If more (as a fraction of total) people of group X apply, then there should be more (as a fraction of total) people of group X that are admitted, assuming <demographic trait that X belongs to> is not a causal factor in admissions.</li>
</ol>
<h2>Approach #4, a test for the existence of an effective target quota, with less data.</h2>
<p>This approach is rather specific to college admissions and I would like to believe that I came up with it first. If not, then at least I can feel good for thinking of it independently. “Since the application season consists of different rounds [early, regular, and waitlist], it should be extremely obvious from the <em>waitlist admitted list</em> if a school were performing demographic balancing to meet a target quota.” <strong>The only data we need for this approach are the <em>demographic trait</em> of students on the [non-waitlist, waitlist] x [accepted, enrolled] lists.</strong></p>
<p>First, let us assume that the quota target is “50:50 gender ratio”. In order to meet this target <em>and</em> maintain a high academic standard overall, Caltech should employ the following strategy during the admissions cycle (they’re smart - I thought of this on the spot, therefore they would know this before they even need to think about it):</p>
<ol>
<li>Before admissions season, advertise Caltech to gender <strong>F</strong> in high schools with competent STEM kids. This is to remedy the historical (and probably current) stereotypes of the school, which naturally hurt <strong>F</strong> application rates.</li>
<li>In early action phase, admit all the best (scientifically promising or other metric) kids that applied. Send less qualified kids to the ‘defer’ box for later. ‘Reject’ the truly unpromising ones. Ignore demographics in this phase.<ol>
<li>Call all of the admitted kids and send them postcards to try and get them to enrol. <a href="https://caltech.typepad.com/caltech_as_it_happens/2016/02/phone-and-postcard-campaign-final-.html">This is a real thing</a> I like it and I help out. Unfortunately, I don’t know how much it helps yield rates - I assume it is decently helpful, given that we’ve been doing it for a couple of years now.</li>
</ol>
</li>
<li>In regular decision phase, combine the ‘defer’ population with the regular applicants, and then select the best kids out of this pool. <strong>Target the previous steps using yield predictions such that the school can admit 10–15% of total class size through the ‘waitlist’</strong>. Send the next tier to the ‘waitlist’, and ‘reject’ the rest. Ignore demographics in this phase.</li>
<li>May 1st. Students have made their university decisions. Calculate total demographic composition of all <em>enrolled</em> students.</li>
<li>Admit people from the waitlist to fill the class <strong>in such a way that the demographic target is optimised, </strong>while <strong>secondarily</strong> maximising the “academic/scientific promise” of the admitted students.</li>
</ol>
<p><strong>Assuming this strategy is deployed, what patterns should we be able to observe </strong>if we have the list of non-waitlist and waitlist admits/enrolled over the years? We <strong>don’t have data on those who applied and were rejected.</strong></p>
<ol>
<li>In a given year, if the number of non-waitlist enrolled students is <em>very close to the maximum</em> class size, then we should expect to see a <em>greater degree of demographic<strong><em>* </em></strong></em>imbalance*. With less room to manoeuvre in the waitlist phase, the admissions office will have a harder time performing balancing. Example:<ol>
<li>Gender ratio is M:F=60:40 before using waitlist. 234 out of 235 spots filled.</li>
<li>There is <strong>only 1 spot remaining </strong>for the waitlist admits due to underprediction in yield rate.</li>
<li>After choosing <strong>1F</strong> from the waitlist, the M:F ratio decreases to (rounded down) 59:41.</li>
</ol>
</li>
<li>In a given year, if a demographic imbalance is large in the non-waitlist enrolled list (step #4 from above), then expect to see a similarly large demographic imbalance in the waitlist-admitted list, except in the other direction. Example:<ol>
<li>Gender ratio is M:F=55:45 before using waitlist. 205 out of 235 spots filled.</li>
<li>There are 30 spots left.</li>
<li>Therefore, admit <strong>4M</strong> and <strong>26F</strong> to achieve a total target ratio of M:F=<strong>117:118~50:50</strong></li>
<li>The M:F ratio on the waitlist admit list is <strong>4:26</strong>.</li>
</ol>
</li>
<li>Due to the nature of the waitlist (it’s a second-choice list versus the regular admissions), the above strategy will necessarily result in a difference in quality between F and M students if significant balancing is performed. This will be detectable by <strong>Approach #1, which requires academic data for all enrolled students</strong>. To use Approach #1, we need exam score/other academic data, which is harder to get.<ol>
<li><em>Criticism: what if the school targets a good demographic balance in <strong><em>*every</em></strong></em> phase of admissions? The M vs F distributions between phases would look roughly the same that way.*</li>
<li>Answer: I believe that Caltech isn’t motivated enough to do that, as it would sacrifice on quality, <em>every</em> phase of the way. <strong>Approach #2 </strong>would likely be able to detect problems, however.</li>
</ol>
</li>
</ol>
<p>If the waitlist operates without demographic balancing, then we should expect to find roughly equal proportions of M:F in every step of the process, after controlling for the applicant M:F ratio in each step. The fact that Caltech admits so few people complicates things, as the sample size is small and could leave room for “random chance” causing our observations. The good news is, we theoretically have multiple years of data.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
    </div>
  </article>
  
    <ul>
         <li>
             <a href="/gaming-hist.html">
                 Previous article: "The Past, Present, and Future of Online Gaming according to Jim"
             </a>
         </li>
     
         <li>
             <a href="/course-game.html">
                 Next article: "The Time Value of Knowledge: Course Selection as a Board Game"
             </a>
         </li>
    </ul>
    </div>
  </div>

  <footer class="footer">
    <div class="container">
<div class="row">
  <ul class="col-sm-6 list-inline">
    <li class="list-inline-item"><a href="/archives.html">Archives</a></li>
    <li class="list-inline-item"><a href="/categories.html">Categories</a></li>
      <li class="list-inline-item"><a href="/tags.html">Tags</a></li>
  </ul>
  <!--
  <p class="col-sm-6 text-sm-right text-muted">
    Generated by <a href="https://github.com/getpelican/pelican" target="_blank">Pelican</a>
    / <a href="https://github.com/nairobilug/pelican-alchemy" target="_blank">&#x2728;</a>
  </p>
  -->
</div>    </div>
  </footer>
</body>

</html>